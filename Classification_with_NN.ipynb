{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb7a10d1-b8c4-47f9-a06a-3c40cd5fa3b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, when, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a96150-2cd6-4aba-984b-dc1cbcdd9ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"DDAM Project\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04586d53-1e60-43a0-be69-53ac8fb1a25a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c667b946-70a3-40e2-9ae9-1793b6cd1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boxscore = spark.read.csv(\"data/boxscore_clean.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08a36f25-27d8-46b3-bf90-9690b642a9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41633\n"
     ]
    }
   ],
   "source": [
    "print(df_boxscore.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4deff412-2294-4577-8f33-74083ef3ef29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Renaming\n",
    "df_boxscore = df_boxscore.withColumnRenamed(\"pos_clean\", \"Position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0fb90f3-8be7-4004-9737-6e20d22a55a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- playerName: string (nullable = true)\n",
      " |-- game_id: integer (nullable = true)\n",
      " |-- teamName: string (nullable = true)\n",
      " |-- FG: integer (nullable = true)\n",
      " |-- FGA: integer (nullable = true)\n",
      " |-- 3P: integer (nullable = true)\n",
      " |-- 3PA: integer (nullable = true)\n",
      " |-- FT: integer (nullable = true)\n",
      " |-- FTA: integer (nullable = true)\n",
      " |-- ORB: integer (nullable = true)\n",
      " |-- DRB: integer (nullable = true)\n",
      " |-- TRB: integer (nullable = true)\n",
      " |-- AST: integer (nullable = true)\n",
      " |-- STL: integer (nullable = true)\n",
      " |-- BLK: integer (nullable = true)\n",
      " |-- TOV: integer (nullable = true)\n",
      " |-- PF: integer (nullable = true)\n",
      " |-- PTS: integer (nullable = true)\n",
      " |-- +/-: integer (nullable = true)\n",
      " |-- isStarter: integer (nullable = true)\n",
      " |-- seasonStartYear: integer (nullable = true)\n",
      " |-- isRegular: integer (nullable = true)\n",
      " |-- Ht: string (nullable = true)\n",
      " |-- Wt: double (nullable = true)\n",
      " |-- MP_seconds: integer (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_boxscore.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f3cf9-476c-4935-9176-3f55e487bb23",
   "metadata": {},
   "source": [
    "## Position one-hot-enconding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a2be45-d808-4e9c-a877-ea35f5ae9cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Position\", outputCol=\"PosNum\", handleInvalid=\"skip\")\n",
    "\n",
    "df_boxscore = indexer.fit(df_boxscore).transform(df_boxscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "009cbaf4-28f5-45c7-bc85-60b0975f34ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+--------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---------+---------------+---------+----+-----+----------+--------+------+-------------+\n",
      "|      playerName|game_id|            teamName| FG|FGA| 3P|3PA| FT|FTA|ORB|DRB|TRB|AST|STL|BLK|TOV| PF|PTS|+/-|isStarter|seasonStartYear|isRegular|  Ht|   Wt|MP_seconds|Position|PosNum|       PosVec|\n",
      "+----------------+-------+--------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---------+---------------+---------+----+-----+----------+--------+------+-------------+\n",
      "|   Desmond Mason|   4577| Seattle SuperSonics|  3|  9|  0|  3|  1|  2|  3|  2|  5|  2|  0|  1|  1|  5|  7|-15|        1|           2000|        1| 6-7|224.0|      1021|       F|   1.0|(2,[1],[1.0])|\n",
      "|Rubén Wolkowyski|   4577| Seattle SuperSonics|  0|  4|  0|  0|  0|  0|  1|  1|  2|  0|  0|  1|  0|  1|  0| -2|        0|           2000|        1|6-10|270.0|       707|       F|   1.0|(2,[1],[1.0])|\n",
      "|  Stromile Swift|   4577| Vancouver Grizzlies|  0|  1|  0|  0|  0|  0|  1|  0|  1|  0|  0|  0|  0|  0|  0|  7|        0|           2000|        1| 6-9|225.0|       534|       F|   1.0|(2,[1],[1.0])|\n",
      "|   Obinna Ekezie|   4578|  Washington Wizards|  0|  2|  0|  0|  0|  0|  1|  2|  3|  2|  0|  0|  0|  2|  0| -7|        0|           2000|        1| 6-9|270.0|       433|       F|   1.0|(2,[1],[1.0])|\n",
      "|    Rafer Alston|   4579|     Milwaukee Bucks|  0|  0|  0|  0|  0|  0|  1|  0|  1|  0|  0|  0|  0|  0|  0|  0|        0|           2000|        1| 6-2|171.0|        20|       G|   0.0|(2,[0],[1.0])|\n",
      "|    Greg Buckner|   4579|    Dallas Mavericks|  2|  7|  0|  0|  3|  6|  8|  3| 11|  0|  1|  0|  0|  4|  7|-10|        0|           2000|        1| 6-4|210.0|      1635|       G|   0.0|(2,[0],[1.0])|\n",
      "| Todd MacCulloch|   4580|  Philadelphia 76ers|  3|  5|  0|  0|  1|  4|  0|  2|  2|  0|  1|  0|  0|  1|  7| -4|        0|           2000|        1| 7-0|280.0|       464|       C|   2.0|    (2,[],[])|\n",
      "|   Jumaine Jones|   4580|  Philadelphia 76ers|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0| -2|        0|           2000|        1| 6-8|218.0|       342|       F|   1.0|(2,[1],[1.0])|\n",
      "|    Pepe Sánchez|   4580|  Philadelphia 76ers|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  2|        0|           2000|        1| 6-4|195.0|       148|       G|   0.0|(2,[0],[1.0])|\n",
      "|   Lavor Postell|   4580|     New York Knicks|  1|  5|  0|  2|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  2|  3|        0|           2000|        1| 6-5|215.0|       623|       G|   0.0|(2,[0],[1.0])|\n",
      "|   Chucky Atkins|   4581|     Detroit Pistons| 10| 25|  1|  7|  0|  2|  1|  4|  5|  7|  2|  0|  0|  0| 21| -1|        1|           2000|        1|5-11|160.0|      2199|       G|   0.0|(2,[0],[1.0])|\n",
      "|  Mateen Cleaves|   4581|     Detroit Pistons|  2|  4|  0|  1|  0|  0|  0|  0|  0|  1|  1|  0|  1|  1|  4| 10|        0|           2000|        1| 6-2|205.0|       681|       G|   0.0|(2,[0],[1.0])|\n",
      "|  Khalid El-Amin|   4582|       Chicago Bulls|  4|  7|  1|  1|  2|  4|  0|  1|  1|  3|  1|  0|  3|  1| 11|-12|        1|           2000|        1|5-10|200.0|      1644|       G|   0.0|(2,[0],[1.0])|\n",
      "|  Michael Ruffin|   4582|       Chicago Bulls|  0|  0|  0|  0|  0|  0|  2|  4|  6|  1|  0|  0|  1|  4|  0| -6|        1|           2000|        1| 6-9|246.0|      1210|       F|   1.0|(2,[1],[1.0])|\n",
      "|    Marcus Fizer|   4582|       Chicago Bulls|  7| 13|  2|  2|  0|  0|  2|  2|  4|  0|  1|  0|  2|  2| 16| -9|        0|           2000|        1| 6-9|262.0|      1455|       F|   1.0|(2,[1],[1.0])|\n",
      "|     A.J. Guyton|   4582|       Chicago Bulls|  0|  1|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0| -1|        0|           2000|        1| 6-1|180.0|       213|       G|   0.0|(2,[0],[1.0])|\n",
      "|Wally Szczerbiak|   4583|Minnesota Timberw...|  2|  8|  0|  0|  0|  0|  2|  2|  4|  5|  0|  0|  2|  3|  4|  6|        1|           2000|        1| 6-7|244.0|      1396|       F|   1.0|(2,[1],[1.0])|\n",
      "|   Steve Francis|   4583|     Houston Rockets|  4| 15|  0|  1|  7|  8|  4|  1|  5|  8|  3|  0|  6|  3| 15| -6|        1|           2000|        1| 6-3|195.0|      2468|       G|   0.0|(2,[0],[1.0])|\n",
      "|   Jason Collier|   4583|     Houston Rockets|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0| -5|        0|           2000|        1| 7-0|260.0|       173|       C|   2.0|    (2,[],[])|\n",
      "| Jonathan Bender|   4584|      Indiana Pacers|  1|  2|  1|  2|  0|  0|  0|  0|  0|  1|  0|  1|  2|  2|  3| -5|        0|           2000|        1|6-11|202.0|       526|       F|   1.0|(2,[1],[1.0])|\n",
      "+----------------+-------+--------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---------+---------------+---------+----+-----+----------+--------+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "onehotencoder_embarked_vector = OneHotEncoder(inputCol=\"PosNum\", outputCol=\"PosVec\")\n",
    "df_boxscore = onehotencoder_embarked_vector.fit(df_boxscore).transform(df_boxscore)\n",
    "df_boxscore.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "776ffef8-d6dd-4928-8528-766628379418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column2drop = ('playerName', 'game_id', 'teamName', 'seasonStartYear', 'Ht', 'Pos', 'Position')\n",
    "df_boxscore = df_boxscore.drop(*column2drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87e25be7-ea01-404c-bc99-c1b40576f704",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FG: integer (nullable = true)\n",
      " |-- FGA: integer (nullable = true)\n",
      " |-- 3P: integer (nullable = true)\n",
      " |-- 3PA: integer (nullable = true)\n",
      " |-- FT: integer (nullable = true)\n",
      " |-- FTA: integer (nullable = true)\n",
      " |-- ORB: integer (nullable = true)\n",
      " |-- DRB: integer (nullable = true)\n",
      " |-- TRB: integer (nullable = true)\n",
      " |-- AST: integer (nullable = true)\n",
      " |-- STL: integer (nullable = true)\n",
      " |-- BLK: integer (nullable = true)\n",
      " |-- TOV: integer (nullable = true)\n",
      " |-- PF: integer (nullable = true)\n",
      " |-- PTS: integer (nullable = true)\n",
      " |-- +/-: integer (nullable = true)\n",
      " |-- isStarter: integer (nullable = true)\n",
      " |-- isRegular: integer (nullable = true)\n",
      " |-- Wt: double (nullable = true)\n",
      " |-- MP_seconds: integer (nullable = true)\n",
      " |-- PosNum: double (nullable = false)\n",
      " |-- PosVec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_boxscore.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912f4c94-0db5-4017-8c4c-48e59052c0c5",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a3073e2-b5d4-4904-977e-382992b9e279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39d8ac46-3802-4606-a69b-94b5b2093673",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', '+/-', 'isStarter', 'isRegular', 'MP_seconds']\n"
     ]
    }
   ],
   "source": [
    "num_col = [item[0] for item in df_boxscore.dtypes if not item[1].startswith('string')]\n",
    "num_col.remove(\"PosNum\")\n",
    "num_col.remove(\"Wt\")\n",
    "num_col.remove(\"PosVec\")\n",
    "print(num_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fae745a-8d01-47bd-b583-178b6186af7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------+------+\n",
      "|features                                                                            |PosNum|\n",
      "+------------------------------------------------------------------------------------+------+\n",
      "|[3.0,9.0,0.0,3.0,1.0,2.0,3.0,2.0,5.0,2.0,0.0,1.0,1.0,5.0,7.0,-15.0,1.0,1.0,1021.0]  |1.0   |\n",
      "|(19,[1,6,7,8,11,13,15,17,18],[4.0,1.0,1.0,2.0,1.0,1.0,-2.0,1.0,707.0])              |1.0   |\n",
      "|(19,[1,6,8,15,17,18],[1.0,1.0,1.0,7.0,1.0,534.0])                                   |1.0   |\n",
      "|(19,[1,6,7,8,9,13,15,17,18],[2.0,1.0,2.0,3.0,2.0,2.0,-7.0,1.0,433.0])               |1.0   |\n",
      "|(19,[6,8,17,18],[1.0,1.0,1.0,20.0])                                                 |0.0   |\n",
      "|[2.0,7.0,0.0,0.0,3.0,6.0,8.0,3.0,11.0,0.0,1.0,0.0,0.0,4.0,7.0,-10.0,0.0,1.0,1635.0] |0.0   |\n",
      "|[3.0,5.0,0.0,0.0,1.0,4.0,0.0,2.0,2.0,0.0,1.0,0.0,0.0,1.0,7.0,-4.0,0.0,1.0,464.0]    |2.0   |\n",
      "|(19,[13,15,17,18],[1.0,-2.0,1.0,342.0])                                             |1.0   |\n",
      "|(19,[9,15,17,18],[2.0,2.0,1.0,148.0])                                               |0.0   |\n",
      "|(19,[0,1,3,9,14,15,17,18],[1.0,5.0,2.0,2.0,2.0,3.0,1.0,623.0])                      |0.0   |\n",
      "|[10.0,25.0,1.0,7.0,0.0,2.0,1.0,4.0,5.0,7.0,2.0,0.0,0.0,0.0,21.0,-1.0,1.0,1.0,2199.0]|0.0   |\n",
      "|(19,[0,1,3,9,10,12,13,14,15,17,18],[2.0,4.0,1.0,1.0,1.0,1.0,1.0,4.0,10.0,1.0,681.0])|0.0   |\n",
      "|[4.0,7.0,1.0,1.0,2.0,4.0,0.0,1.0,1.0,3.0,1.0,0.0,3.0,1.0,11.0,-12.0,1.0,1.0,1644.0] |0.0   |\n",
      "|(19,[6,7,8,9,12,13,15,16,17,18],[2.0,4.0,6.0,1.0,1.0,4.0,-6.0,1.0,1.0,1210.0])      |1.0   |\n",
      "|[7.0,13.0,2.0,2.0,0.0,0.0,2.0,2.0,4.0,0.0,1.0,0.0,2.0,2.0,16.0,-9.0,0.0,1.0,1455.0] |1.0   |\n",
      "|(19,[1,3,15,17,18],[1.0,1.0,-1.0,1.0,213.0])                                        |0.0   |\n",
      "|[2.0,8.0,0.0,0.0,0.0,0.0,2.0,2.0,4.0,5.0,0.0,0.0,2.0,3.0,4.0,6.0,1.0,1.0,1396.0]    |1.0   |\n",
      "|[4.0,15.0,0.0,1.0,7.0,8.0,4.0,1.0,5.0,8.0,3.0,0.0,6.0,3.0,15.0,-6.0,1.0,1.0,2468.0] |0.0   |\n",
      "|(19,[12,15,17,18],[1.0,-5.0,1.0,173.0])                                             |2.0   |\n",
      "|[1.0,2.0,1.0,2.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,2.0,2.0,3.0,-5.0,0.0,1.0,526.0]    |1.0   |\n",
      "+------------------------------------------------------------------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=num_col, outputCol=\"features\")\n",
    "\n",
    "output_dataset = assembler.transform(df_boxscore)\n",
    "\n",
    "classificationData = output_dataset.select(\"features\", \"PosNum\")\n",
    "\n",
    "classificationData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed8557f2-62b9-4b58-8b98-fbb65c38ca05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = classificationData.randomSplit([0.7, 0.3],seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39eab193-3f3b-453e-b018-b15afbe694db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------+------+\n",
      "|features                                                                            |PosNum|\n",
      "+------------------------------------------------------------------------------------+------+\n",
      "|(19,[],[])                                                                          |1.0   |\n",
      "|(19,[0,1,2,3,4,5,6,8,14,17,18],[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,645.0])     |1.0   |\n",
      "|(19,[0,1,2,3,4,5,7,8,14,17,18],[5.0,7.0,3.0,3.0,4.0,6.0,1.0,1.0,17.0,1.0,846.0])    |1.0   |\n",
      "|(19,[0,1,2,3,4,5,9,13,14,17,18],[4.0,9.0,1.0,2.0,6.0,7.0,1.0,1.0,15.0,1.0,1327.0])  |1.0   |\n",
      "|(19,[0,1,2,3,4,5,9,14,15,17,18],[1.0,1.0,1.0,1.0,2.0,2.0,1.0,5.0,-1.0,1.0,1031.0])  |0.0   |\n",
      "|(19,[0,1,2,3,4,5,9,14,15,17,18],[1.0,1.0,1.0,1.0,2.0,2.0,1.0,5.0,2.0,1.0,260.0])    |0.0   |\n",
      "|(19,[0,1,2,3,4,5,9,14,15,17,18],[1.0,7.0,1.0,4.0,2.0,2.0,5.0,5.0,-17.0,1.0,1049.0]) |0.0   |\n",
      "|(19,[0,1,2,3,4,5,9,14,15,17,18],[1.0,7.0,1.0,4.0,2.0,2.0,5.0,5.0,-17.0,1.0,1049.0]) |1.0   |\n",
      "|(19,[0,1,2,3,4,5,9,14,15,17,18],[2.0,2.0,1.0,1.0,2.0,2.0,1.0,7.0,10.0,1.0,425.0])   |0.0   |\n",
      "|(19,[0,1,2,3,4,5,9,14,15,17,18],[2.0,2.0,2.0,2.0,2.0,2.0,1.0,8.0,4.0,1.0,929.0])    |0.0   |\n",
      "|(19,[0,1,2,3,4,5,9,14,15,17,18],[2.0,3.0,1.0,2.0,2.0,2.0,1.0,7.0,1.0,1.0,275.0])    |0.0   |\n",
      "|(19,[0,1,2,3,4,5,9,14,15,17,18],[3.0,4.0,1.0,2.0,2.0,2.0,1.0,9.0,3.0,1.0,744.0])    |0.0   |\n",
      "|(19,[0,1,2,3,4,5,9,14,15,17,18],[3.0,6.0,1.0,1.0,1.0,2.0,2.0,8.0,2.0,1.0,774.0])    |1.0   |\n",
      "|(19,[0,1,2,3,4,5,9,14,15,17,18],[3.0,6.0,2.0,3.0,1.0,1.0,1.0,9.0,-2.0,1.0,952.0])   |0.0   |\n",
      "|(19,[0,1,2,3,4,5,9,14,15,17,18],[3.0,7.0,1.0,2.0,2.0,2.0,2.0,9.0,-7.0,1.0,656.0])   |0.0   |\n",
      "|(19,[0,1,2,3,4,5,9,14,15,17,18],[3.0,8.0,1.0,5.0,3.0,3.0,2.0,10.0,-7.0,1.0,1127.0]) |0.0   |\n",
      "|(19,[0,1,2,3,4,5,9,14,15,17,18],[4.0,6.0,1.0,1.0,1.0,2.0,3.0,10.0,3.0,1.0,1224.0])  |0.0   |\n",
      "|(19,[0,1,2,3,4,5,9,14,15,17,18],[6.0,8.0,1.0,1.0,2.0,4.0,1.0,15.0,6.0,1.0,1016.0])  |0.0   |\n",
      "|(19,[0,1,2,3,4,5,10,12,14,17,18],[2.0,4.0,1.0,1.0,4.0,4.0,1.0,1.0,9.0,1.0,653.0])   |0.0   |\n",
      "|(19,[0,1,2,3,4,5,10,14,15,17,18],[1.0,3.0,1.0,1.0,2.0,2.0,1.0,5.0,-3.0,1.0,343.0])  |0.0   |\n",
      "|(19,[0,1,2,3,4,5,10,14,15,17,18],[2.0,4.0,1.0,2.0,1.0,2.0,1.0,6.0,3.0,1.0,476.0])   |0.0   |\n",
      "|(19,[0,1,2,3,4,5,10,14,15,17,18],[2.0,4.0,1.0,2.0,4.0,4.0,1.0,9.0,-3.0,1.0,289.0])  |2.0   |\n",
      "|(19,[0,1,2,3,4,5,10,14,15,17,18],[5.0,6.0,1.0,1.0,1.0,1.0,1.0,12.0,3.0,1.0,862.0])  |1.0   |\n",
      "|(19,[0,1,2,3,4,5,12,13,14,17,18],[2.0,6.0,1.0,4.0,1.0,2.0,2.0,1.0,6.0,1.0,738.0])   |2.0   |\n",
      "|(19,[0,1,2,3,4,5,12,13,14,17,18],[6.0,11.0,1.0,2.0,4.0,5.0,1.0,3.0,17.0,1.0,1975.0])|0.0   |\n",
      "|(19,[0,1,2,3,4,5,12,14,15,17,18],[1.0,3.0,1.0,1.0,2.0,2.0,1.0,5.0,-1.0,1.0,339.0])  |1.0   |\n",
      "|(19,[0,1,2,3,4,5,12,14,15,17,18],[1.0,3.0,1.0,3.0,2.0,2.0,2.0,5.0,-6.0,1.0,799.0])  |0.0   |\n",
      "|(19,[0,1,2,3,4,5,12,14,15,17,18],[5.0,7.0,4.0,6.0,2.0,2.0,3.0,16.0,6.0,1.0,1130.0]) |0.0   |\n",
      "|(19,[0,1,2,3,4,5,13,14,15,17,18],[1.0,2.0,1.0,1.0,3.0,4.0,1.0,6.0,1.0,1.0,760.0])   |1.0   |\n",
      "|(19,[0,1,2,3,4,5,13,14,15,17,18],[1.0,2.0,1.0,2.0,1.0,2.0,1.0,4.0,-5.0,1.0,234.0])  |0.0   |\n",
      "|(19,[0,1,2,3,4,5,13,14,15,17,18],[1.0,4.0,1.0,2.0,1.0,2.0,1.0,4.0,-1.0,1.0,342.0])  |0.0   |\n",
      "|(19,[0,1,2,3,4,5,13,14,15,17,18],[1.0,4.0,1.0,2.0,7.0,9.0,2.0,10.0,8.0,1.0,1216.0]) |0.0   |\n",
      "|(19,[0,1,2,3,4,5,13,14,15,17,18],[1.0,4.0,1.0,3.0,2.0,2.0,1.0,5.0,-12.0,1.0,693.0]) |0.0   |\n",
      "|(19,[0,1,2,3,4,5,13,14,15,17,18],[1.0,6.0,1.0,5.0,2.0,2.0,3.0,5.0,-2.0,1.0,859.0])  |1.0   |\n",
      "|(19,[0,1,2,3,4,5,13,14,15,17,18],[2.0,3.0,2.0,3.0,2.0,2.0,2.0,8.0,1.0,1.0,568.0])   |0.0   |\n",
      "|(19,[0,1,2,3,4,5,13,14,15,17,18],[2.0,4.0,1.0,2.0,1.0,2.0,3.0,6.0,-6.0,1.0,718.0])  |0.0   |\n",
      "|(19,[0,1,2,3,4,5,13,14,15,17,18],[3.0,5.0,3.0,4.0,2.0,2.0,1.0,11.0,2.0,1.0,1307.0]) |0.0   |\n",
      "|(19,[0,1,2,3,4,5,13,14,15,17,18],[3.0,7.0,2.0,4.0,1.0,2.0,1.0,9.0,6.0,1.0,1029.0])  |0.0   |\n",
      "|(19,[0,1,2,3,4,5,13,14,17,18],[3.0,5.0,2.0,4.0,2.0,2.0,1.0,10.0,1.0,644.0])         |0.0   |\n",
      "|(19,[0,1,2,3,4,5,14,15,16,17,18],[2.0,9.0,2.0,5.0,2.0,2.0,8.0,-19.0,1.0,1.0,1391.0])|0.0   |\n",
      "|(19,[0,1,2,3,4,5,14,15,17,18],[1.0,1.0,1.0,1.0,1.0,1.0,4.0,2.0,1.0,123.0])          |1.0   |\n",
      "|(19,[0,1,2,3,4,5,14,15,17,18],[1.0,1.0,1.0,1.0,1.0,2.0,4.0,3.0,1.0,148.0])          |0.0   |\n",
      "|(19,[0,1,2,3,4,5,14,15,17,18],[1.0,1.0,1.0,1.0,2.0,2.0,5.0,2.0,1.0,146.0])          |0.0   |\n",
      "|(19,[0,1,2,3,4,5,14,15,17,18],[1.0,1.0,1.0,1.0,2.0,3.0,5.0,-2.0,1.0,178.0])         |0.0   |\n",
      "|(19,[0,1,2,3,4,5,14,15,17,18],[1.0,4.0,1.0,3.0,2.0,2.0,5.0,4.0,1.0,429.0])          |0.0   |\n",
      "|(19,[0,1,2,3,4,5,14,15,17,18],[1.0,5.0,1.0,3.0,3.0,4.0,6.0,-6.0,1.0,1129.0])        |0.0   |\n",
      "|(19,[0,1,2,3,4,5,14,15,17,18],[2.0,2.0,1.0,1.0,3.0,4.0,8.0,7.0,1.0,399.0])          |1.0   |\n",
      "|(19,[0,1,2,3,4,5,14,15,17,18],[2.0,3.0,1.0,2.0,1.0,3.0,6.0,4.0,1.0,305.0])          |1.0   |\n",
      "|(19,[0,1,2,3,4,5,14,15,17,18],[2.0,4.0,1.0,2.0,1.0,4.0,6.0,-5.0,1.0,549.0])         |0.0   |\n",
      "|(19,[0,1,2,3,4,5,14,15,17,18],[2.0,5.0,1.0,1.0,2.0,2.0,7.0,3.0,1.0,637.0])          |1.0   |\n",
      "+------------------------------------------------------------------------------------+------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac8bb7-28fc-46e1-8199-6ea7d1f76e68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b6989db-41fe-499b-b2ac-1d4dfdcaa4c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"PosNum\", featuresCol=\"features\")\n",
    "\n",
    "dt = dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e5a61fc-bc7f-443f-89fe-73444d27106c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(19, {3: 0.4584, 6: 0.1168, 7: 0.0249, 8: 0.1153, 9: 0.2077, 11: 0.0543, 13: 0.0082, 16: 0.0145})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff50b822-0b33-4d44-b7d9-2896a11a4614",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "|            features|PosNum|       rawPrediction|         probability|prediction|\n",
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "|          (19,[],[])|   0.0|[1050.0,832.0,385.0]|[0.46316718129686...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|  [2493.0,348.0,6.0]|[0.87565858798735...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0| [2021.0,545.0,35.0]|[0.77700884275278...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|  [2493.0,348.0,6.0]|[0.87565858798735...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0| [2021.0,545.0,35.0]|[0.77700884275278...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   1.0|  [131.0,138.0,19.0]|[0.45486111111111...|       1.0|\n",
      "|(19,[0,1,2,3,4,5,...|   1.0|  [131.0,138.0,19.0]|[0.45486111111111...|       1.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1312.0,646.0,105.0]|[0.63596703829374...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1312.0,646.0,105.0]|[0.63596703829374...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1312.0,646.0,105.0]|[0.63596703829374...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1312.0,646.0,105.0]|[0.63596703829374...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   1.0|[1312.0,646.0,105.0]|[0.63596703829374...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1312.0,646.0,105.0]|[0.63596703829374...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1312.0,646.0,105.0]|[0.63596703829374...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   1.0|[1312.0,646.0,105.0]|[0.63596703829374...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   2.0|[1312.0,646.0,105.0]|[0.63596703829374...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1312.0,646.0,105.0]|[0.63596703829374...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1312.0,646.0,105.0]|[0.63596703829374...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1312.0,646.0,105.0]|[0.63596703829374...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   1.0|[1312.0,646.0,105.0]|[0.63596703829374...|       0.0|\n",
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = dt.transform(testData)\n",
    "\n",
    "predictions.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "726a52d7-66e5-4cc6-baed-8ee0725e0bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5814176245210728\n",
      "Test Error = 0.418582\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"PosNum\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82e10ba2-c373-40b3-83b8-bbe06d770b71",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o353.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 33) (192.168.1.8 executor driver): java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 2.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1223)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1225)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1218)\r\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\r\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\r\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\r\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:201)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:170)\r\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 2.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1223)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gabri\\Desktop\\DDAM\\Project\\Classification.ipynb Cell 25\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/DDAM/Project/Classification.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m gbt \u001b[39m=\u001b[39m GBTClassifier(labelCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPosNum\u001b[39m\u001b[39m\"\u001b[39m, featuresCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m, maxIter\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/DDAM/Project/Classification.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/DDAM/Project/Classification.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m gbt\u001b[39m.\u001b[39;49mfit(trainingData)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/DDAM/Project/Classification.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Make predictions on the test data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/DDAM/Project/Classification.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtransform(testData)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[0;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o353.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 33) (192.168.1.8 executor driver): java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 2.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1223)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1225)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1218)\r\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\r\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\r\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\r\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:201)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:170)\r\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 2.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1223)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Create a GBTClassifier instance\n",
    "gbt = GBTClassifier(labelCol=\"PosNum\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train the model\n",
    "model = gbt.fit(trainingData)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "# You can also view the feature importances\n",
    "feature_importances = model.featureImportances\n",
    "print(\"Feature Importances:\", feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b374d-8d12-406a-bc02-91b6ab5c7458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for future use\n",
    "#model.save(\"path_to_save_model\")\n",
    "\n",
    "# Load the saved model\n",
    "#loaded_model = GBTClassifier.load(\"path_to_saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5830938697318008\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "layers = [19, 20, 10, 3]  # Define the layers of the neural network\n",
    "mlp = MultilayerPerceptronClassifier(labelCol=\"PosNum\", featuresCol=\"features\", layers=layers, seed=1234)\n",
    "\n",
    "model = mlp.fit(trainingData)\n",
    "\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"PosNum\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "|            features|PosNum|       rawPrediction|         probability|prediction|\n",
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "|          (19,[],[])|   0.0|[0.22675666490199...|[0.34289128806061...|       1.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1.40293352602737...|[0.73677563936177...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1.37340800021238...|[0.72848575524147...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1.40659596981844...|[0.73781480705336...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1.40506285250744...|[0.73737955780179...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   1.0|[0.79855463231135...|[0.53155658310424...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   1.0|[1.26167844519160...|[0.69468219473418...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1.40385532799476...|[0.73703750720679...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1.19087804004461...|[0.67253631433828...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1.40659142366287...|[0.73781350430930...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1.40614160076206...|[0.73768957571390...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   1.0|[0.85415444435007...|[0.55571906862180...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1.40185929237823...|[0.73647021813982...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[0.54744727857455...|[0.44386471668086...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   1.0|[1.57431623293767...|[0.77615298721804...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   2.0|[1.12628101370286...|[0.65103192048483...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1.45592971098976...|[0.75251934958779...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1.40071336802682...|[0.73618763091926...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   0.0|[1.47061279704378...|[0.75750141584011...|       0.0|\n",
      "|(19,[0,1,2,3,4,5,...|   1.0|[1.21393917367474...|[0.67968393530250...|       0.0|\n",
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultilayerPerceptronClassificationModel: uid=MultilayerPerceptronClassifier_dd3785011acc, numLayers=4, numClasses=3, numFeatures=19"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
